---
layout: page
permalink: /Research/
---


<p align = "justify"> My research focuses on developing theoretical foundations and computational methods that enable autonomous systems to operate in uncertain, complex, and potentially adversarial domains with verifiable safety and performance guarantees. </p>

The following are some of the projects on which I have been working during the past few years.


***

<center> <h3>Planning in Adversarial Environments</h3> </center>
<p align = "justify"> Consider a mobile robot that aims to complete a mission in a  <span style="color:green">stochastic environment</span> while being  <span style="color:green">observed by adversaries</span>. How can the robot <span style="color:green">protect its mission-critical information</span> from adversaries with unknown locations and capabilities? How would the robot's  <span style="color:green">imperfect sensor measurements</span> affect its ability to protect the mission-critical information? Would the robot improve its performance if it had some <span style="color:green">side information</span> regarding the adversaries' perception or prediction capabilities? In this project, I propose information-theoretic measures to <span style="color:maroon">rigorously express the robot's objective</span>, establish several theoretical results that <span style="color:maroon">characterize the scenarios</span> in which the robot can achieve its objective, and <span style="color:maroon">develop efficient algorithms</span> to synthesize strategies that leak minimum information to adversaries regarding the robot's mission and trajectories. </p>
<span style="line-height: 0;"> _Related Publications:_ </span>
<p style = "align : justify;  margin-left: 20px; margin-top: -1em"><b>Entropy Maximization for Markov Decision Processes Under Temporal Logic Constraints</b>, Y. Savas, M. Ornik, M. Cubuktepe,
 M. O. Karabag, U. Topcu, IEEE Transaction on Automatic Control, 2019  <a href='https://arxiv.org/abs/1807.03223'>[Link]</a>  </p>
<p style = "align : justify;  margin-left: 20px; margin-top: -1em"><b>Unpredictable Planning Under Partial Observability</b>, M. Hibbard, Y. Savas, B. Wu, T. Tanaka, U. Topcu, IEEE Conference on Decision and Control, 2019 <a href='https://arxiv.org/abs/1903.07665'>[Link]</a></p>
<p style = "align : justify;  margin-left: 20px; margin-top: -1em"><b>Minimizing the Information Leakage Regarding High-Level Task Specifications</b>, M. Hibbard, Y. Savas, Z. Xu, U. Topcu, IFAC World Congress, 2020 <a href='https://www.sciencedirect.com/science/article/pii/S2405896320330287'>[Link]</a></p>

***

<center> <h3>Sequential Incentive Design</h3> </center>
<p align = "justify">Consider a principal that aims to <span style="color:green">modify an agent's behavior</span> through a sequence of <span style="color:green">incentive offers</span>. For example, an online retailer may aim to convince a customer to purchase more products over time by offering a sequence of discounts, or a ridesharing company may aim to convince a driver to be present at a certain location during rush hour by offering additional cash income for a series of rides. Is it possible to modify the agent's behavior if the agent has an <span style="color:green">unknown intrinsic motivation</span>? If possible, what is the <span style="color:green">minimum cost of behavior manipulation</span>? What are the <span style="color:green">memory requirements</span> for the principal to minimize its cost? Does there exist an <span style="color:green">efficient algorithm</span> to compute an incentive sequence that minimizes the cost to the principal? In this project, I establish several theoretical results that  <span style="color:maroon">characterize the computational complexity</span> of the considered behavior manipulation problem and <span style="color:maroon">illustrate the value of memory</span> for the principal. I also <span style="color:maroon">propose several tractable algorithms</span> to synthesize incentive sequences that enable the principal to induce the desired agent behavior. </p>
<span style="line-height: 0;">_Related Publications:_</span>
<p style = "align : justify;  margin-left: 20px; margin-top: -1em"><b> Incentive Design for Temporal Logic Objectives</b>, Y. Savas, V. Gupta, M. Ornik, L. J. Ratliff, U. Topcu, IEEE Conference on Decision and Control, 2019 <a href='https://arxiv.org/abs/1903.07752'>[Link]</a></p>
<p style = "align : justify;  margin-left: 20px; margin-top: -1em"><b>On the Complexity of Sequential Incentive Design</b>, Y. Savas, V. Gupta, U. Topcu,  IEEE Transactions on Automatic Control, <i>accepted</i> <a href='https://arxiv.org/abs/2007.08548'>[Link]</a></p>

***

<center> <h3>Secure Communication in the Presence of Eavesdroppers</h3> </center>
<p align = "justify"> Consider a group of robots that aim to communicate a <span style="color:green">confidential message</span> with a base station in the presence of adversaries that eavesdrop on the wireless transmission. How can the robots <span style="color:green">ensure the security of communication under power constraints without using encryption</span> methods that rely on the existence of a secret key shared between the robots and the base station? How would the <span style="color:green">adversaries' exact location information</span> affect the security of communication? In this project, I <span style="color:maroon">propose an efficient algorithm</span>, based on the notion of physical-layer security and convex optimization, that ensures the security of communication with <span style="color:maroon">probabilistic guarantees</span> even when the exact locations of the adversaries are unknown to the robots. </p>
<span style="line-height: 0;">_Related Publications:_</span>
<p style = "align : justify;  margin-left: 20px; margin-top: -1em"> <b>Physical-Layer Security via Distributed Beamforming in the Presence of Adversaries with Unknown Locations</b>, Y. Savas, A. Hashemi, A. P. Vinod, B. M. Sadler, U. Topcu, IEEE International Conference on Acoustics, Speech, and Signal Processing, 2021 <a href='https://arxiv.org/abs/2103.00630'>[Link]</a></p>

***



